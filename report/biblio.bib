@article{
doi:10.1126/science.aam9744,
author = {Charles E. Leiserson  and Neil C. Thompson  and Joel S. Emer  and Bradley C. Kuszmaul  and Butler W. Lampson  and Daniel Sanchez  and Tao B. Schardl },
title = {There’s plenty of room at the Top: What will drive computer performance after Moore’s law?},
journal = {Science},
volume = {368},
number = {6495},
pages = {eaam9744},
year = {2020},
doi = {10.1126/science.aam9744},
URL = {https://www.science.org/doi/abs/10.1126/science.aam9744},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aam9744},
abstract = {The doubling of the number of transistors on a chip every 2 years, a seemly inevitable trend that has been called Moore's law, has contributed immensely to improvements in computer performance. However, silicon-based transistors cannot get much smaller than they are today, and other approaches should be explored to keep performance growing. Leiserson et al. review recent examples and argue that the most promising place to look is at the top of the computing stack, where improvements in software, algorithms, and hardware architecture can bring the much-needed boost. Science, this issue p. eaam9744 The miniaturization of semiconductor transistors has driven the growth in computer performance for more than 50 years. As miniaturization approaches its limits, bringing an end to Moore’s law, performance gains will need to come from software, algorithms, and hardware. We refer to these technologies as the “Top” of the computing stack to distinguish them from the traditional technologies at the “Bottom”: semiconductor physics and silicon-fabrication technology. In the post-Moore era, the Top will provide substantial performance gains, but these gains will be opportunistic, uneven, and sporadic, and they will suffer from the law of diminishing returns. Big system components offer a promising context for tackling the challenges of working at the Top.}}

@inproceedings{x_heep, title={X-HEEP: An Open-Source, Configurable and Extendible RISC-V Microcontroller}, ISBN={979-8-4007-0140-5}, url={https://infoscience.epfl.ch/handle/20.500.14299/197196}, DOI={10.1145/3587135.3591431}, abstractNote={In this work, we present eXtendible Heterogeneous Energy-Efficient Platform (X-HEEP), a configurable and extendible single-core RISC-V-based ultra-low-power microcontroller. X-HEEP can be used standalone as a low-cost microcontroller, or it can be integrated into existing platforms to act like a peripheral subsystem, or it can be extended and customized with external peripherals and accelerators.}, publisher={New York}, author={Schiavone, Pasquale Davide and Machetti, Simone and Peon Quiros, Miguel and Miranda Calero, José Angel and Denkinger, Benoît Walter and Müller, Christoph Thomas and Rodríguez Álvarez, Rubén and Nasturzio, Saverio and Atienza Alonso, David}, year={2023}, month={may}, pages={379–380}, journal={Proceedings Of The 20Th Acm International Conference On Computing Frontiers 2023, Cf 2023} }


@article{tvm,
  author       = {Tianqi Chen and
                  Thierry Moreau and
                  Ziheng Jiang and
                  Haichen Shen and
                  Eddie Q. Yan and
                  Leyuan Wang and
                  Yuwei Hu and
                  Luis Ceze and
                  Carlos Guestrin and
                  Arvind Krishnamurthy},
  title        = {{TVM:} End-to-End Optimization Stack for Deep Learning},
  journal      = {CoRR},
  volume       = {abs/1802.04799},
  year         = {2018},
  url          = {http://arxiv.org/abs/1802.04799},
  eprinttype    = {arXiv},
  eprint       = {1802.04799},
  timestamp    = {Sat, 17 Dec 2022 01:15:27 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1802-04799.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{exo,
author = {Ikarashi, Yuka and Bernstein, Gilbert Louis and Reinking, Alex and Genc, Hasan and Ragan-Kelley, Jonathan},
title = {Exocompilation for productive programming of hardware accelerators},
year = {2022},
isbn = {9781450392655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519939.3523446},
doi = {10.1145/3519939.3523446},
abstract = {High-performance kernel libraries are critical to exploiting accelerators and specialized instructions in many applications. Because compilers are difficult to extend to support diverse and rapidly-evolving hardware targets, and automatic optimization is often insufficient to guarantee state-of-the-art performance, these libraries are commonly still coded and optimized by hand, at great expense, in low-level C and assembly. To better support development of high-performance libraries for specialized hardware, we propose a new programming language, Exo, based on the principle of exocompilation: externalizing target-specific code generation support and optimization policies to user-level code. Exo allows custom hardware instructions, specialized memories, and accelerator configuration state to be defined in user libraries. It builds on the idea of user scheduling to externalize hardware mapping and optimization decisions. Schedules are defined as composable rewrites within the language, and we develop a set of effect analyses which guarantee program equivalence and memory safety through these transformations. We show that Exo enables rapid development of state-of-the-art matrix-matrix multiply and convolutional neural network kernels, for both an embedded neural accelerator and x86 with AVX-512 extensions, in a few dozen lines of code each.},
booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {703–718},
numpages = {16},
keywords = {hardware accelerators, instruction abstraction, program optimization, scheduling, user-extensible backend \& scheduling, user-schedulable languages},
location = {San Diego, CA, USA},
series = {PLDI 2022}
}

@article{conv1d_survey,
title = {1D convolutional neural networks and applications: A survey},
journal = {Mechanical Systems and Signal Processing},
volume = {151},
pages = {107398},
year = {2021},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2020.107398},
url = {https://www.sciencedirect.com/science/article/pii/S0888327020307846},
author = {Serkan Kiranyaz and Onur Avci and Osama Abdeljaber and Turker Ince and Moncef Gabbouj and Daniel J. Inman},
keywords = {Artificial Neural Networks, Machine learning, Deep learning, Convolutional neural networks, Structural health monitoring, Condition monitoring, Arrhythmia detection and identification, Fault detection, Structural damage detection},
abstract = {During the last decade, Convolutional Neural Networks (CNNs) have become the de facto standard for various Computer Vision and Machine Learning operations. CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and millions of parameters have the ability to learn complex objects and patterns providing that they can be trained on a massive size visual database with ground-truth labels. With a proper training, this unique ability makes them the primary tool for various engineering applications for 2D signals such as images and video frames. Yet, this may not be a viable option in numerous applications over 1D signals especially when the training data is scarce or application specific. To address this issue, 1D CNNs have recently been proposed and immediately achieved the state-of-the-art performance levels in several applications such as personalized biomedical data classification and early diagnosis, structural health monitoring, anomaly detection and identification in power electronics and electrical motor fault detection. Another major advantage is that a real-time and low-cost hardware implementation is feasible due to the simple and compact configuration of 1D CNNs that perform only 1D convolutions (scalar multiplications and additions). This paper presents a comprehensive review of the general architecture and principals of 1D CNNs along with their major engineering applications, especially focused on the recent progress in this field. Their state-of-the-art performance is highlighted concluding with their unique properties. The benchmark datasets and the principal 1D CNN software used in those applications are also publicly shared in a dedicated website. While there has not been a paper on the review of 1D CNNs and its applications in the literature, this paper fulfills this gap.}
}