{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import meta_schedule as ms\n",
    "from tvm.tir import schedule as sch\n",
    "from tvm.relax.transform import LegalizeOps\n",
    "from tvm.script import ir as I, relax as R, tir as T\n",
    "from tvm.tir import TensorIntrin\n",
    "from tvm import tir\n",
    "from tvm.ir.module import IRModule\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@I.ir_module\n",
    "class Matmul:\n",
    "    @R.function\n",
    "    def main(\n",
    "        a: R.Tensor((128, 128), \"float32\"), b: R.Tensor((128, 128), \"float32\")\n",
    "    ) -> R.Tensor((128, 128), \"float32\"):\n",
    "        out: R.Tensor((128, 128), \"float32\") = R.matmul(a, b)\n",
    "        return out\n",
    "legalized_matmul = LegalizeOps()(Matmul)\n",
    "\n",
    "@T.prim_func\n",
    "def before_tensorize(\n",
    "    A: T.Buffer((128, 128), \"float32\"),\n",
    "    B: T.Buffer((128, 128), \"float32\"),\n",
    "    C: T.Buffer((128, 128), \"float32\"),\n",
    ") -> None:\n",
    "    # body\n",
    "    # with T.block(\"root\")\n",
    "    for i_0, j_0, k_0, i_1, j_1, k_1 in T.grid(8, 8, 8, 16, 16, 16):\n",
    "        with T.block(\"matmul\"):\n",
    "            vi = T.axis.spatial(128, i_0 * 16 + i_1)\n",
    "            vj = T.axis.spatial(128, j_0 * 16 + j_1)\n",
    "            vk = T.axis.reduce(128, k_0 * 16 + k_1)\n",
    "            T.writes(C[vi, vj])\n",
    "            T.reads(A[vi, vk], B[vj, vk])\n",
    "            #with T.init():\n",
    "            #    C[vi, vj] = T.float32(0)\n",
    "            C[vi, vj] = A[vi, vk] * B[vj, vk]\n",
    "\n",
    "#sched = tir.Schedule(before_tensorize)\n",
    "#sched.mod.show()\n",
    "#block = sched.get_block(\"matmul\", func_name=\"before_tensorize\")\n",
    "#i, j, k = sched.get_loops(block)\n",
    "#i0, i1 = sched.split(i, [None, 16])\n",
    "#j0, j1 = sched.split(j, [None, 16])\n",
    "#k0, k1 = sched.split(k, [None, 16])\n",
    "#sched.reorder(i0, j0, k0, i1, j1, k1)\n",
    "#sched.mod.show(black_format=False, obj_to_underline=[sched.get_sref(block).stmt.init])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@T.prim_func\n",
    "def mma_desc(a: T.handle, b: T.handle, c: T.handle) -> None:\n",
    "    A = T.match_buffer(a, (16, 16), align=64, offset_factor=1)\n",
    "    B = T.match_buffer(b, (16, 16), align=64, offset_factor=1)\n",
    "    C = T.match_buffer(c, (16, 16), align=64, offset_factor=1)\n",
    "\n",
    "    with T.block(\"root\"):\n",
    "        T.reads(A[0 : 16, 0 : 16], B[0 : 16, 0 : 16])\n",
    "        T.writes(C[0 : 16, 0 : 16])\n",
    "        for i, j, k in T.grid(16, 16, 16):\n",
    "            with T.block(\"update\"):\n",
    "                vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n",
    "                # C[vi, vj] +\n",
    "                C[vi, vj] =  A[vi, vk] * B[vj, vk]\n",
    "\n",
    "##@T.prim_func\n",
    "##def mma_intrin(a: T.handle, b: T.handle, c: T.handle) -> None:\n",
    "##    A = T.match_buffer(a, (16, 16), align=128, offset_factor=1)\n",
    "##    B = T.match_buffer(b, (16, 16), align=128, offset_factor=1)\n",
    "##    C = T.match_buffer(c, (16, 16), align=128, offset_factor=1)\n",
    "##\n",
    "##    with T.block(\"root\"):\n",
    "##        T.reads(C[0 : 16, 0 : 16], A[0 : 16, 0 : 16], B[0 : 16, 0 : 16])\n",
    "##        T.writes(C[0 : 16, 0 : 16])\n",
    "##        T.call_extern(\"float32\", \"gemm\", C.access_ptr(\"w\"), A.access_ptr(\"r\"), B.access_ptr(\"r\"))\n",
    "    \n",
    "@T.prim_func\n",
    "def mma_intrin(a: T.handle, b: T.handle, c: T.handle) -> None:\n",
    "    A = T.match_buffer(a, (16, 16), align=64, offset_factor=1)\n",
    "    B = T.match_buffer(b, (16, 16), align=64, offset_factor=1)\n",
    "    C = T.match_buffer(c, (16, 16), align=64, offset_factor=1)\n",
    "\n",
    "    with T.block(\"root\"):\n",
    "        T.reads(A[0 : 16, 0 : 16], B[0 : 16, 0 : 16])\n",
    "        T.writes(C[0 : 16, 0 : 16])\n",
    "        T.call_extern(\"float32\", \"gemm\", C.access_ptr(\"w\"), A.access_ptr(\"r\"), B.access_ptr(\"r\"))\n",
    "\n",
    "TensorIntrin.register(\"test_mma_intrin\", mma_desc, mma_intrin)\n",
    "#def gemm_impl():\n",
    "#    asm_code = \"\"\"\n",
    "#    /* Copied from matrix_insn.S in QEMU testcases */\n",
    "#    .text\n",
    "#    .align  2\n",
    "#    .global test_fmmacc_s_4x4\n",
    "#    .type   test_fmmacc_s_4x4, @function\n",
    "#    test_fmmacc_s_4x4:\n",
    "#        addi  t0,x0,0x10\n",
    "#        li t3, 0x00100404\n",
    "#        mcfg x0, t3\n",
    "#\n",
    "#        mldw m0, t0, (a0)\n",
    "#        mldw m1, t0, (a1)\n",
    "#\n",
    "#        li       t6,  0x00000010\n",
    "#        csrw xmcsr,t6\n",
    "#\n",
    "#        fmmacc.s m2, m1, m0\n",
    "#\n",
    "#        mstw m2, t0, (a3)\n",
    "#\n",
    "#        ret\n",
    "#            .size   test_fmmacc_s_4x4, .-test_fmmacc_s_4x4\n",
    "#\n",
    "#    \"\"\"\n",
    "#    from tvm.contrib import utils, clang\n",
    "#\n",
    "#    temp = utils.tempdir()\n",
    "#    ll_path = temp.relpath(\"temp.ll\")\n",
    "#    # Create LLVM ir from c source code\n",
    "#    ll_code = clang.create_llvm(cc_code, output=ll_path)\n",
    "#    return ll_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import tir as T\n",
      "\n",
      "@T.prim_func\n",
      "def before_tensorize(A: T.Buffer((128, 128), \"float32\"), B: T.Buffer((128, 128), \"float32\"), C: T.Buffer((128, 128), \"float32\")):\n",
      "    # with T.block(\"root\"):\n",
      "    for i_0, j_0, k_0 in T.grid(8, 8, 8):\n",
      "        with T.block(\"matmul_o\"):\n",
      "            vi_o, vj_o, vk_o = T.axis.remap(\"SSR\", [i_0, j_0, k_0])\n",
      "            T.reads(A[vi_o * 16:vi_o * 16 + 16, vk_o * 16:vk_o * 16 + 16], B[vj_o * 16:vj_o * 16 + 16, vk_o * 16:vk_o * 16 + 16])\n",
      "            T.writes(C[vi_o * 16:vi_o * 16 + 16, vj_o * 16:vj_o * 16 + 16])\n",
      "            A_1 = T.match_buffer(A[vi_o * 16:vi_o * 16 + 16, vk_o * 16:vk_o * 16 + 16], (16, 16), offset_factor=1)\n",
      "            B_1 = T.match_buffer(B[vj_o * 16:vj_o * 16 + 16, vk_o * 16:vk_o * 16 + 16], (16, 16), offset_factor=1)\n",
      "            C_1 = T.match_buffer(C[vi_o * 16:vi_o * 16 + 16, vj_o * 16:vj_o * 16 + 16], (16, 16), offset_factor=1)\n",
      "            T.call_extern(\"float32\", \"gemm\", T.tvm_access_ptr(T.type_annotation(\"float32\"), C_1.data, C_1.elem_offset, 256, 2), T.tvm_access_ptr(T.type_annotation(\"float32\"), A_1.data, A_1.elem_offset, 256, 1), T.tvm_access_ptr(T.type_annotation(\"float32\"), B_1.data, B_1.elem_offset, 256, 1))\n"
     ]
    }
   ],
   "source": [
    "#update = sched.get_block(\"matmul\", func_name=\"matmul\")\n",
    "#_, _, _, i1, _, _ = sched.get_loops(update)\n",
    "#sched.tensorize(i1, \"test_mma_intrin\")\n",
    "#print(sched.mod[\"main\"].script())\n",
    "\n",
    "sched = tir.Schedule(before_tensorize)\n",
    "update = sched.get_block(\"matmul\")\n",
    "_, _, _, i1, _, _ = sched.get_loops(update)\n",
    "sched.tensorize(i1, \"test_mma_intrin\")\n",
    "print(sched.mod[\"main\"].script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IR_module = IRModule({\"main\": sched})\n",
    "mod = tvm.build(IR_module, target=\"llvm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
